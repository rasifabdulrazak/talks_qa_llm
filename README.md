# ğŸ¤– PDF Questionear

- A production-ready FastAPI application for PDF document question-answering using Large Language Models (LLMs). Upload a PDF, ask questions, and get intelligent answers based on the document content.

ğŸ“„ Used Models and Reason to use
===================
- gpt-4o-mini => Fast response, moderately accurate
- gpt-4-turbo => Fast Response (but less compared to mini), Accurate response
- I used gpt-4o-mini and gpt-4-turbo for answering PDF-based questions. The gpt-4o-mini model gives very fast responsesâ€”usually around 2.x seconds for the same PDFâ€”while still maintaining good accuracy. In comparison, gpt-4-turbo takes slightly longer, around 3.x seconds, but provides more accurate and reliable answers. Together, they offer a balance between speed and precision depending on the requirement.


âœ¨ Features
===================
- ğŸ” JWT Authentication - Secure user registration and login
- ğŸ“„ PDF Processing - Extract text from PDF documents
- ğŸ¤– LLM Integration - Support for OpenAI GPT
- âš¡ Smart Caching - Fast responses for repeated questions
- ğŸ”„ Streaming Support - Real-time streaming responses
- ğŸ³ Docker Ready - Containerized deployment with Docker Compose
- ğŸ—„ï¸ PostgreSQL - Reliable database with Alembic migrations
- ğŸ“Š API Documentation - Auto-generated Swagger UI

ğŸ—ï¸ Architecture
===================
```bash
talks_qa_llm
â”œâ”€â”€alembic
â”‚   â”œâ”€â”€versions
â”‚   â”‚   â””â”€â”€417a23b64bf9_create_users_table.py
â”‚   â”œâ”€â”€env.py
â”‚   â”œâ”€â”€README
â”‚   â””â”€â”€script.py.mako
â”œâ”€â”€app
â”‚   â”œâ”€â”€api
â”‚   â”‚   â”œâ”€â”€__init__.py
â”‚   â”‚   â”œâ”€â”€auth.py
â”‚   â”‚   â”œâ”€â”€bot.py
â”‚   â”‚   â””â”€â”€deps.py
â”‚   â”œâ”€â”€core
â”‚   â”‚   â”œâ”€â”€__init__.py
â”‚   â”‚   â”œâ”€â”€config.py
â”‚   â”‚   â”œâ”€â”€redis.py
â”‚   â”‚   â”œâ”€â”€security.py
â”‚   â”‚   â””â”€â”€utils.py
â”‚   â”œâ”€â”€db
â”‚   â”‚   â”œâ”€â”€__init__.py
â”‚   â”‚   â””â”€â”€session.py
â”‚   â”œâ”€â”€models
â”‚   â”‚   â”œâ”€â”€__init__.py
â”‚   â”‚   â””â”€â”€user.py
â”‚   â”œâ”€â”€schema
â”‚   â”‚   â”œâ”€â”€__init__.py
â”‚   â”‚   â”œâ”€â”€bot.py
â”‚   â”‚   â””â”€â”€user.py
â”‚   â”œâ”€â”€__init__.py
â”‚   â””â”€â”€main.py
â”œâ”€â”€alembic.ini
â”œâ”€â”€docker-compose.yml
â”œâ”€â”€Dockerfile
â”œâ”€â”€README.md
â”œâ”€â”€requirements.txt
â”œâ”€â”€sample.env
â”œâ”€â”€.dockerignore
â””â”€â”€.gitignore
```



ğŸš€ Quick Start
==================

Docker & Docker Compose (recommended)
Python 3.11+ (for local development)
PostgreSQL 15+ (for local development)
OpenAI API Key for llm apis

Option 1: Docker Setup (Recommended) â­

1. Clone the Repository

```console
git clone https://github.com/rasifabdulrazak/talks_qa_llm
cd talks_qa_llm
```

2. Create Environment File

```bash
cp sample.env

```
Edit .env with your configuration

4. Build and Start Services
```bash
# Build and start all services
docker-compose up --build -d

# Check logs
docker-compose logs -f app

# Check if services are running
docker-compose ps
```

5. Run Database Migrations
```bash
# Create initial migration
docker-compose exec app alembic revision --autogenerate -m "Initial migration"

# Apply migrations
docker-compose exec app alembic upgrade head
```

6. Access the API

- API Documentation: http://localhost:8000/docs
- Alternative Docs: http://localhost:8000/redoc
- Health Check: http://localhost:8000/system-check/


Option 2: Local Development Setup

1. Create Virtual Environment
```bash
python -m venv venv

# On Linux/Mac
source venv/bin/activate

# On Windows
venv\Scripts\activate
```

2. Install Dependencies
```bash
pip install -r requirements.txt
```

3. Install and Setup PostgreSQL URL in .env
```bash
DATABASE_URL=postgresql://your_username:your_password@your_host:your_port/db_name
```

4. Run Migrations
```bash
alembic revision --autogenerate -m "Initial migration"
alembic upgrade head
```

5. Start the Application
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

ğŸ“– API Usage
================
1. Register a New User
```bash
curl -X POST "http://localhost:8000/api/auth/register/" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "user@example.com",
    "name": "John Doe",
    "password": "securepassword123"
  }'
```
Response:
```json
{
  "id": 1,
  "email": "user@example.com",
  "name": "John Doe",
  "created_at": "2024-11-14T10:30:00",
  "updated_at": null
}
```
2. Login

```bash
curl -X POST "http://localhost:8000/api/auth/login/" \
  -H "Content-Type: application/json" \
  -d "email=user@example.com&password=securepassword123"
```

Response:
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer"
}
```

3. Ask a Question About a PDF
```bash
curl -X POST "http://localhost:8000/api/bot/ask/" \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
  -F "file=@/path/to/document.pdf" \
  -F "question=What is this document about?" \
```

Response:
```json
{
  "question": "What is this document about?",
  "answer": "This document is a Resume of mohammed Rasif ...",
  "pdf_filename": "document.pdf",
  "extracted_text_length": 15420,
  "processing_time": 2.35,
  "timestamp": "2024-11-14T10:35:00"
}
```

4. Return Stream data ,Ask a Question About a PDF ,Will return streaming response which may fine for frontend apps and best User Experience
```bash
curl -X POST "http://localhost:8000/api/bot/ask-stream/" \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
  -F "file=@/path/to/document.pdf" \
  -F "question=What is this document about?" \
```

Response:
```bash
data: Moh

data: ammed

data:  Ras

data: if

data:  C

data:  has
etc....
```

5. Logout

```bash
curl -X POST "http://localhost:8000/api/auth/logout/" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_ACCESS_TOKEN" \
```

Response:
```json
{
  "message": "Successfully logged out. Token has been blacklisted."
}
```

ğŸ“Œ Project Summary
===================
- This project delivers a robust PDF-based Q&A system powered by an LLM. It provides two authorised endpointsâ€”one for normal responses and one for real-time streamingâ€”offering flexibility between speed and interactivity. The architecture is clean, modular, and production-ready, with clear separation of concerns across services, utilities, and API layers. It ensures reliable PDF extraction, optimized LLM handling, and efficient streaming.

- The system has been tested using multiple models, where gpt-4o-mini provides extremely fast responses (~2 seconds), while gpt-4-turbo offers higher accuracy (~3 seconds). This balance allows developers to choose between speed and precision based on the use case.

ğŸ”— My Essential Links
====================
```bash
ğŸŒ Portfolio

ğŸ‘‰ https://portfolio.pyrasif.com

ğŸ’¼ LinkedIn

ğŸ‘‰ https://www.linkedin.com/in/mohammed-rasif-056419211

ğŸ’» GitHub

ğŸ‘‰ https://github.com/rasifabdulrazak
```